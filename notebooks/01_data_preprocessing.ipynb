{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model is ready.\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "def ensure_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    try:\n",
    "        import spacy\n",
    "        spacy.load(model_name)\n",
    "    except Exception:\n",
    "        print(f\"Installing spaCy model: {model_name}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\n",
    "\n",
    "ensure_spacy_model()\n",
    "print(\"spaCy model is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Imports and basic setup\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (515738, 17)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Load raw data\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Path to the raw CSV\n",
    "RAW_DATA = Path(\"../data/raw_data\") / \"Hotel_Reviews copy 2.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df_raw = pd.read_csv(RAW_DATA, low_memory=False)\n",
    "\n",
    "# Quick shape check\n",
    "print(\"Raw shape:\", df_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After clean shape: (499845, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Review</th>\n",
       "      <th>encoded_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am so angry that i made this post available...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No real complaints the hotel was great great...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rooms are nice but for elderly a bit difficul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Total_Review  encoded_review\n",
       "0   I am so angry that i made this post available...               0\n",
       "1    No real complaints the hotel was great great...               1\n",
       "2   Rooms are nice but for elderly a bit difficul...               1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# label creation\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Replace placeholders with empty strings (avoid \"No Positive\"/\"No Negative\" in text)\n",
    "df[\"Positive_Review\"] = df[\"Positive_Review\"].replace(\"No Positive\", \"\", regex=False)\n",
    "df[\"Negative_Review\"] = df[\"Negative_Review\"].replace(\"No Negative\", \"\", regex=False)\n",
    "\n",
    "# Combine into one review text; fill missing with empty strings\n",
    "df[\"Total_Review\"] = df[\"Negative_Review\"].fillna(\"\") + \" \" + df[\"Positive_Review\"].fillna(\"\")\n",
    "\n",
    "# Binary target: 1 if score >= 7, else 0\n",
    "df[\"encoded_review\"] = (df[\"Reviewer_Score\"] >= 7).astype(int)\n",
    "\n",
    "# Keep only what we need\n",
    "df = df[[\"Total_Review\", \"encoded_review\"]]\n",
    "\n",
    "# Drop exact duplicate rows, reset index\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"After clean shape:\", df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working set shape: (200000, 2)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# sample a subset for faster processing\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "N_SAMPLES = 200_000  # change to None to use all rows\n",
    "\n",
    "if N_SAMPLES is not None and N_SAMPLES < len(df):\n",
    "    df_sample = df.sample(n=N_SAMPLES, random_state=123)\n",
    "else:\n",
    "    df_sample = df.copy()\n",
    "\n",
    "print(\"Working set shape:\", df_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Text preprocessing\n",
    "#   - clean_text() does lowercase + simple regex cleanup\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Settings\n",
    "USE_LEMMATIZATION = True   # set False if you want faster, no-lemmatization run\n",
    "BATCH_SIZE        = 1000\n",
    "N_PROC            = None   # None -> use ~half your CPU cores\n",
    "\n",
    "# Precompiled regex for speed\n",
    "RE_CONTRACTIONS = [\n",
    "    (re.compile(r\"\\b(can't)\\b\", re.I), \"cannot\"),\n",
    "    (re.compile(r\"\\b(won't)\\b\", re.I), \"will not\"),\n",
    "    (re.compile(r\"n't\\b\", re.I), \" not\"),\n",
    "    (re.compile(r\"'re\\b\", re.I), \" are\"),\n",
    "    (re.compile(r\"'ve\\b\", re.I), \" have\"),\n",
    "    (re.compile(r\"'ll\\b\", re.I), \" will\"),\n",
    "    (re.compile(r\"'d\\b\", re.I), \" would\"),\n",
    "    (re.compile(r\"\\bI'm\\b\", re.I), \"I am\"),\n",
    "    (re.compile(r\"'s\\b\", re.I), \" is\"),\n",
    "    (re.compile(r\"'t\\b\", re.I), \" not\"),\n",
    "]\n",
    "RE_PUNCT    = re.compile(r\"[^\\w\\s]\")\n",
    "RE_DIGIT    = re.compile(r\"\\b\\d+\\b\")\n",
    "RE_NUMWORDS = re.compile(\n",
    "    r\"\\b(one|two|three|four|five|six|seven|eight|nine|ten|\"\n",
    "    r\"first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\\b\", re.I\n",
    ")\n",
    "RE_SINGLE   = re.compile(r\"\\b[a-zA-Z]\\b\")\n",
    "RE_SPACE    = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(x: str) -> str:\n",
    "    \"\"\"Lowercase, expand contractions, strip punctuation/digits/noise.\"\"\"\n",
    "    if not isinstance(x, str):\n",
    "        return \"\"\n",
    "    for patt, repl in RE_CONTRACTIONS:\n",
    "        x = patt.sub(repl, x)\n",
    "    x = x.lower()\n",
    "    x = RE_PUNCT.sub(\" \", x)      # remove punctuation\n",
    "    x = RE_DIGIT.sub(\" \", x)      # remove standalone digits\n",
    "    x = RE_NUMWORDS.sub(\" \", x)   # remove spelled-out numbers (optional)\n",
    "    x = RE_SINGLE.sub(\" \", x)     # remove single letters\n",
    "    x = RE_SPACE.sub(\" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def preprocess_text_column(df_in: pd.DataFrame, text_col: str) -> pd.Series:\n",
    "    \"\"\"Apply clean_text()\"\"\"\n",
    "    texts = df_in[text_col].fillna(\"\").astype(str).map(clean_text).tolist()\n",
    "\n",
    "    if not USE_LEMMATIZATION:\n",
    "        return pd.Series(texts, index=df_in.index)\n",
    "\n",
    "    # Load spaCy English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"attribute_ruler\"])\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "    # Decide parallel workers\n",
    "    n_proc = max(1, (os.cpu_count() or 2) // 2) if N_PROC is None else N_PROC\n",
    "\n",
    "    out = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=n_proc),\n",
    "                    total=len(texts), desc=\"lemmatizing\"):\n",
    "        # keep alphabetic lemmas, drop spaces/punct/numbers, drop stopwords\n",
    "        kept = [t.lemma_ for t in doc\n",
    "                if (not t.is_space) and (not t.is_punct) and (not t.like_num)\n",
    "                and t.lemma_ and (t.lemma_ not in stop_words)]\n",
    "        out.append(\" \".join(kept))\n",
    "\n",
    "    return pd.Series(out, index=df_in.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lemmatizing:   0%|          | 0/200000 [00:00<?, ?it/s]/Users/lucasvercauteren/Desktop/gehaalde vakken/Master eur/seminar/final paper/hotel-reviews-sentiment/.conda/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "lemmatizing:   0%|          | 840/200000 [00:02<06:51, 483.60it/s]/Users/lucasvercauteren/Desktop/gehaalde vakken/Master eur/seminar/final paper/hotel-reviews-sentiment/.conda/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "lemmatizing:   1%|          | 1651/200000 [00:03<03:01, 1091.57it/s]/Users/lucasvercauteren/Desktop/gehaalde vakken/Master eur/seminar/final paper/hotel-reviews-sentiment/.conda/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "lemmatizing:   1%|▏         | 2935/200000 [00:03<01:45, 1859.67it/s]/Users/lucasvercauteren/Desktop/gehaalde vakken/Master eur/seminar/final paper/hotel-reviews-sentiment/.conda/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "lemmatizing:   2%|▏         | 3742/200000 [00:04<01:50, 1770.91it/s]/Users/lucasvercauteren/Desktop/gehaalde vakken/Master eur/seminar/final paper/hotel-reviews-sentiment/.conda/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "lemmatizing: 100%|██████████| 200000/200000 [01:21<00:00, 2450.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Review</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>encoded_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429328</th>\n",
       "      <td>Only a short stay Reception was extremely he...</td>\n",
       "      <td>short stay reception extremely helpful advice ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481203</th>\n",
       "      <td>It location far from historic center The park...</td>\n",
       "      <td>location far historic center parking free room...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344085</th>\n",
       "      <td>Nothing  Perfect Location and staff</td>\n",
       "      <td>perfect location staff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181804</th>\n",
       "      <td>More english tv channels  The croissants are ...</td>\n",
       "      <td>english tv channels croissants amazing omelettes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321093</th>\n",
       "      <td>Room looked into an internal courtyard and wa...</td>\n",
       "      <td>room looked internal courtyard dark issue loca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Total_Review  \\\n",
       "429328    Only a short stay Reception was extremely he...   \n",
       "481203   It location far from historic center The park...   \n",
       "344085                Nothing  Perfect Location and staff   \n",
       "181804   More english tv channels  The croissants are ...   \n",
       "321093   Room looked into an internal courtyard and wa...   \n",
       "\n",
       "                                      preprocessed_review  encoded_review  \n",
       "429328  short stay reception extremely helpful advice ...               1  \n",
       "481203  location far historic center parking free room...               1  \n",
       "344085                             perfect location staff               1  \n",
       "181804   english tv channels croissants amazing omelettes               1  \n",
       "321093  room looked internal courtyard dark issue loca...               1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Apply the preprocessing to the working dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "df_sample[\"preprocessed_review\"] = preprocess_text_column(df_sample, \"Total_Review\")\n",
    "\n",
    "df_sample[[\"Total_Review\", \"preprocessed_review\", \"encoded_review\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed dataset to: /Users/lucasvercauteren/Desktop/gehaalde vakken/Master eur/seminar/final paper/hotel_reviews_sent_python_notebook/data/clean_data/clean_preprocessed_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Save to clean_data (CSV)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "OUT_DIR = Path(\"../data/clean_data\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PATH = OUT_DIR / \"clean_preprocessed_reviews.csv\"\n",
    "df_sample.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "print(\"Saved preprocessed dataset to:\", OUT_PATH.resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
